{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96bcdbb3",
   "metadata": {},
   "source": [
    "# <center>Tracking Illegal Fishing Using Machine Learning</center>\n",
    "\n",
    "##### <center>STUDENT NAME : ARUL RAYMONDS GEORGE JOSEPH</center>\n",
    "##### <center>STUDENT ID: C00278718</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09fd080",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "1. [Introduction](#first-bullet)\n",
    "2. [Businiess Understanding](#second-bullet)\n",
    "3. [Data Understanding](#third-bullet)<br>\n",
    "    3.1 [Load Data](#ld)<br>\n",
    "    3.2 [Data Description](#desc)<br>\n",
    "    3.3 [Visualization](#vs)<br>\n",
    "4. [Data Preparation](#fourth-bullet)<br>\n",
    "    4.1 [Fishing Activity](#fa)<br>\n",
    "    4.2 [Data Cleaning](#dc)<br>\n",
    "    4.3 [Vessel movements](#vm)<br>\n",
    "5. [Modelling](#fifth-bullet)<br>\n",
    "    5.1 [Random Forest](#rf)<br>\n",
    "    5.2 [K-Nearest neighbor](#knn)<br>\n",
    "    5.3 [Gaussian Naive Bayes](#gnb)<br>\n",
    "    5.4 [Logistic Regression](#lr)<br>\n",
    "    5.5 [Neural Networks](#nn)<br>\n",
    "6. [Evaluation](#sixth-bullet)\n",
    "7. [Deployments](#seventh-bullet)\n",
    "8. [Discussion](#eighth-bullet)\n",
    "9. [Results](#nineth-bullet)\n",
    "10. [References](#tenth-bullet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d21f00",
   "metadata": {},
   "source": [
    "## 1. Introduction <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "The world's 12% of the protein intake comes from seafood by the fishing industry globally(Sans, P. and Combris, P., 2015)(What does the world eat? - Sustainable Fisheries UW, 2022). Additionally, the demand for seafood is increasing due to economic availability in developing nations and consumption of exotic fishes in developed countries is on the rise. Also, the fishing industry generates employment for millions of people. So the pressure on the fishing industry is increasing which leads to overfishing, poaching, human rights abuse and other illegal activities. Furthermore, overfishing greatly contributes to the reducing fish stocks of many fish species due to its demand and rarity. So, many governments and non-profit organisations like Global Fishing Watch (GFW), google, Skytruth and Spire are creating tools to actively monitor fishing activities that generate public awareness and help policymakers to take action against illegal activities. \n",
    "<br><br>\n",
    "In this project, we will try to understand and predict fishing activities by different vessel types involved in fishing across the globe by using anonymized open source data provided by GFW. Furthermore, using multiple machine learning(ML) algorithms and techniques to predict fishing activities. Moreover, improving and tweaking the accuracy of the machine learning model's predictions to generate models for new data. Finally, we will discuss on how different ML techniques produces better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61555e21",
   "metadata": {},
   "source": [
    "## 2. Business Understanding <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "Global Fishing Watch, Google, Skytruth, Oceana and many governments like the USA and Europe are working together to create a system of information gathering on the fishing industry to address many issues such as illegal fishing, human trafficking and fishing stock estimations. For this reason, GFW created a platform to compile data involving fishing vessels' movement and activity through monitoring VMS (Vessel Monitoring System) and AIS (Automatic Identification System). Additionally, GFW provides a website platform to visualise historical fishing and non-fishing vessel movements in the sea (GFW | Map, 2022). Furthermore, GFW collects AIS data and satellite imagery from different satellite providers. Also, Google has partnered with GFW to run machine learning models to predict fishing activities from various data sources and compile that to provide an open-source database on vessels. So, in this project, the data is downloaded from GFW open-source database (Global Fishing Watch | Data download portal, 2022).\n",
    "\n",
    "In this project, the data comprises anonymized AIS records of fishing vessels like trawlers, drifting longline, purse seines and trollers. However, only trawlers and drifting longliners data are used in the project since, the feautures are the same in all data files and machines performance bottleneck hinders the complete use of all files. However, the same models can be used with other files after minor data cleaning efforts.\n",
    "\n",
    "Overall, the objective of this project is to identify whether a vessel is fishing or not using ML models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792fe75",
   "metadata": {},
   "source": [
    "## 3. Data understanding <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "\n",
    "The data is downloaded from GFW [website](https://globalfishingwatch.org/data-download/) (Global Fishing Watch | Data download portal, 2022). which contains seven files separated by the vessel type. In this section, data is loaded, visualised and analysed. For simplicity, trawlers.csv file is chosen since all files have the same features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069246d6",
   "metadata": {},
   "source": [
    "First lets import pandas and numpy to load the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy import distance\n",
    "from global_land_mask import globe\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "plt.rcParams['figure.figsize'] = [6, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6038a96b",
   "metadata": {},
   "source": [
    "### 3.1 Load Data from datapath <a class=\"anchor\" id=\"ld\"></a>\n",
    "I have uploded the data and the code in [Github](https://github.com/raymond571/ML-assignment) in compressed format. Please unzip the file and place it under data foler in the project directoy.\n",
    "<br>Note: for drifting_longline use the same notebook and change df = pd.read_csv(datapath+drifting_longlines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c300ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"data/0dab1200-c004-11ec-8a45-f167084fd93d/\"\n",
    "purse_seines = \"purse_seines.csv\"\n",
    "unknown = \"unknown.csv\"\n",
    "trollers = \"trollers.csv\"\n",
    "trawlers = \"trawlers.csv\"\n",
    "pole_and_line = \"pole_and_line.csv\"\n",
    "fixed_gear = \"fixed_gear.csv\"\n",
    "drifting_longlines = \"drifting_longlines.csv\"\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%0.4f' % x)\n",
    "df = pd.read_csv(datapath+purse_seines)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "369dba36",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b41b4ad9",
   "metadata": {},
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffbceb9",
   "metadata": {},
   "source": [
    "### 3.2 Description of attributes in the data file: <a class=\"anchor\" id=\"desc\"></a>\n",
    "As we can see from above, we have 9 attributes in this 'source' is not used since its just tells us which organisation validated this data.<br>\n",
    "Also, all the datatypes are floats by deafult form the database so there is no need of any conversion in data types.\n",
    "<br> \n",
    "\n",
    "* mmsi: Anonymized vessel identifier\n",
    "* timestamp: Unix timestamp\n",
    "* distance_from_shore: Distance from shore (meters)\n",
    "* distance_from_port: Distance from port (meters)\n",
    "* speed: Vessel speed (knots)\n",
    "* course: Vessel course\n",
    "* lat: Latitude in decimal degrees\n",
    "* lon: Longitude in decimal degrees\n",
    "* is_fishing: Label indicating fishing activity.\n",
    "    0 = Not fishing <br>\n",
    "    >0 = Fishing. Data values between 0 and 1 indicate the average score for the position if scored by multiple people.<br>\n",
    "    -1 = No data <br>\n",
    "* source: The training data batch. Data was prepared by GFW, Dalhousie, and a crowd sourcing campaign. False positives are marked as false_positives."
   ]
  },
  {
   "cell_type": "raw",
   "id": "edebf59f",
   "metadata": {},
   "source": [
    "df.describe().apply(lambda s: s.apply('{0:.5f}'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc62e2c",
   "metadata": {},
   "source": [
    "### 3.3 Visualization <a class=\"anchor\" id=\"vs\"></a>\n",
    "Corellation heatmap of attributes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8984fdb",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "sns.heatmap(df.corr());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc5611",
   "metadata": {},
   "source": [
    "Above heatmap we can observe there's a high correlation with distance_from_port and distance_from_shore. So we will be poping distance_from_shore as both are having similar values and meaning. Also, we can observe mmsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda8984",
   "metadata": {},
   "source": [
    "## 4. Data Preparation <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13c936",
   "metadata": {},
   "source": [
    "### 4.1 Analysing is_fishing atrribute <a class=\"anchor\" id=\"fa\"></a>\n",
    "is_fishing = -1; not available <br>\n",
    "is_fishing = 0; not fishing <br>\n",
    "is_fishing > 0; possiblithy of fishing activity <br> <br>\n",
    "Lets find out unique values of is_fishing is captured"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f89a23c",
   "metadata": {},
   "source": [
    "df[\"is_fishing\"].unique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95aa46df",
   "metadata": {},
   "source": [
    "df[df[\"is_fishing\"]==-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3ef71",
   "metadata": {},
   "source": [
    "### 4.2 Data cleaning <a class=\"anchor\" id=\"dc\"></a>\n",
    "There are 7 unique values. We do not need -1 since it has no data on fishing activity identifier. So, we will clean the data. Additionally, create new columns from timestamp like year, month, day and hour, since timestamp is a float number which doesnt provide much informations for the ML models. Also, filter records with null or emplty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column for datetime for convinence\n",
    "df[\"datetime\"] = pd.to_datetime(df['timestamp'],unit='s')\n",
    "df = df.dropna()\n",
    "df = df[df[\"is_fishing\"] != -1]\n",
    "df[\"year\"] = df[\"datetime\"].dt.year\n",
    "df[\"month\"] = df[\"datetime\"].dt.month\n",
    "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "df[\"day\"] = df[\"datetime\"].dt.day\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef34d820",
   "metadata": {},
   "source": [
    "This is a cleaning for a special case which i have found during plotting the geographic map with the data. The mmsi value: 186746307373264 have false location in the land rather than ocean. so removing this mmsi from data will reduce errors in prediction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b139443b",
   "metadata": {},
   "source": [
    "%pip install global-land-mask\n",
    "%conda install -c conda-forge geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e97f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a location coordinate is in land or ocean\n",
    "def land(row):\n",
    "    if globe.is_land(row[\"lat\"],row[\"lon\"]):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0        \n",
    "df[\"land\"]= df.apply(land,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2863f",
   "metadata": {},
   "source": [
    "As mentioned earlier, we will remove distance_from_shore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed813258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for fishing vessel location in ocean\n",
    "ocean_cover = df[df[\"land\"]==0].copy()\n",
    "ocean_cover.drop(['land','distance_from_shore'], axis=1, inplace=True)\n",
    "\n",
    "fishing_vessels = ocean_cover.copy()\n",
    "\n",
    "#get distance between two consecutive vessels with same mmsi\n",
    "def get_dist(dataframe):\n",
    "    dist = [0]\n",
    "    for i in range(len(dataframe)-1):\n",
    "        if dataframe.iloc[i+1,0] == dataframe.iloc[i,0]:\n",
    "            cod1 = (dataframe.iloc[i,5],dataframe.iloc[i,6])\n",
    "            cod2 = (dataframe.iloc[i+1,5],dataframe.iloc[i+1,6])\n",
    "            dist.append( distance.geodesic(cod1, cod2).km )\n",
    "        else:\n",
    "            dist.append(0)\n",
    "    return dist\n",
    "\n",
    "# Group by mmsi and sort with date to get the distance\n",
    "grp_by = fishing_vessels.sort_values(\"datetime\").groupby(\"mmsi\")\n",
    "pddf = grp_by.apply(lambda x: x) \n",
    "pddf[\"dist\"] = get_dist(pddf)\n",
    "fishing_vessels = pddf.copy()\n",
    "\n",
    "fishing_vessels.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d86e0f0",
   "metadata": {},
   "source": [
    "df[df[\"is_fishing\"]!=-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0f3c0",
   "metadata": {},
   "source": [
    "we have 175320 rows of data to train and test our ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fishing vessel error location in land\n",
    "land_cover = df[df[\"land\"]==1].copy()\n",
    "land_cover.drop(['land','distance_from_shore'], axis=1, inplace=True)\n",
    "grp_by = land_cover.sort_values(\"datetime\").groupby(\"mmsi\")\n",
    "pddf = grp_by.apply(lambda x: x)\n",
    "pddf[\"dist\"] = get_dist(pddf)\n",
    "pddf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d76b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf = pddf.copy()\n",
    "def get_dist2(dataframe):\n",
    "    global dataf\n",
    "    sum = 0\n",
    "    for i in range(len(dataframe)-1):\n",
    "        if (dataframe.iloc[i+1,9] - dataframe.iloc[i,9]).days <=1 and dataframe.iloc[i,14] >1500 and dataframe.iloc[i+1,0] == dataframe.iloc[i,0]:\n",
    "            sum+=1\n",
    "            dataf = dataf.append(dataframe.iloc[i,])\n",
    "    print(sum)\n",
    "get_dist2(fishing_vessels)\n",
    "\n",
    "# fishing_vessels[fishing_vessels[\"dist\"]>1500].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709d6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf.drop_duplicates()\n",
    "# dataf.to_csv(datapath+\"purse_seine_land.csv\")\n",
    "dataf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fishing_vessels.hist(layout=(5,3), figsize=(15,15), bins = 100)\n",
    "# fishing_vessels[\"day\"].hist(figsize=(6,2), bins = 100)\n",
    "fishing_vessels[fishing_vessels[\"is_fishing\"]==0][\"day\"].hist(figsize=(6,2), bins = 100)\n",
    "plt.title(\"Distribution of fishing activities in a month\", loc = 'left')\n",
    "plt.xlabel(\"day\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "fishing_vessels[fishing_vessels[\"is_fishing\"]==0][\"month\"].hist(figsize=(6,2), bins = 100)\n",
    "plt.title(\"Distribution of fishing activities in a year\", loc = 'left')\n",
    "plt.xlabel(\"month\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a11833",
   "metadata": {},
   "outputs": [],
   "source": [
    "del [[grp_by,pddf,dataf]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e1fdd",
   "metadata": {},
   "source": [
    "Visualising the cleaned data with pair plot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a9bfa43",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "g = sns.pairplot(fishing_vessels, vars=[\"course\",\"speed\",\"distance_from_port\",\"year\",\"month\",\"hour\",\"day\",\"lat\",\"lon\",\"is_fishing\",\"dist\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60577593",
   "metadata": {},
   "source": [
    "### 4.3 Vessel movements  <a class=\"anchor\" id=\"vm\"></a>\n",
    "Using geographic ploting using plotly we can see the different vessels movement on the sea over the time"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1799575e",
   "metadata": {},
   "source": [
    "import plotly.express as px\n",
    "plt.rcParams['figure.figsize'] = [12, 10]\n",
    "fig = px.line_geo(dataf,lat='lat',lon='lon', color = \"mmsi\", hover_name=\"datetime\",projection =\"azimuthal equal area\")#, \\\n",
    "#animation_frame = fishing_location[\"datetime\"][:500].astype(str))\n",
    "fig.update_layout(title = 'World map', title_x=0.5)\n",
    "fig.show()\n",
    "#fishing_location[\"mmsi\"] = fishing_location[\"mmsi\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06bb95a",
   "metadata": {},
   "source": [
    "Below is the special case data cleaning for drifting_longline data file. Please enable this code while loading drifting_longline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da55a2d2",
   "metadata": {},
   "source": [
    "#df = df[df[\"mmsi\"] != 186746307373264.0000]\n",
    "val = [33266086194351,2713472518343262,204349732031005]\n",
    "fishing_vessels = fishing_vessels[fishing_vessels.mmsi.isin(val) == False]\n",
    "\n",
    "fishing_vessels.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660039f2",
   "metadata": {},
   "source": [
    "## 5. Modelling <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "\n",
    "In this section we are going to train ML models and evaluate the algorithms performance metrics.\n",
    "Machine Leanring models uses:\n",
    "1. Random Forest Classifier/Regressor\n",
    "2. K-nerarest neighbours\n",
    "3. Naive Bayes\n",
    "4. Logistic Regression\n",
    "5. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d393b3",
   "metadata": {},
   "source": [
    "setting features and target of the models used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"mmsi\",\"timestamp\",\"distance_from_port\",\"speed\",\"course\",\"lat\",\"lon\",\"month\",\"hour\",\"day\",\"dist\"]\n",
    "target = [\"is_fishing\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f581f",
   "metadata": {},
   "source": [
    "Importing machine learning ,metrics and validation libraries from scikit learn.<br>\n",
    "Loading features and target into variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "X = fishing_vessels[features]\n",
    "y = fishing_vessels[target]\n",
    "lab = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99d0c1",
   "metadata": {},
   "source": [
    "### 5.1 Random forest <a class=\"anchor\" id=\"rf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c7832f",
   "metadata": {},
   "source": [
    "To optimize the random forest algorithm we have to find the optimal parameters.<br>\n",
    "\n",
    "1. Find the optimal n value(number of decision trees) of the random forest.\n",
    "2. Find optimal max_depth (maximum depth of the tree)\n",
    "3. Find the optimal min_smaple_split (node splits)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8eab69",
   "metadata": {},
   "source": [
    "1. Find n_estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096cb0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "y_transformed = lab.fit_transform(y.values.flatten())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y_transformed, test_size=0.25)\n",
    "\n",
    "a=[]\n",
    "for estimator in n_estimators:\n",
    "    clf=RandomForestClassifier(n_estimators=estimator,min_samples_split=2,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    a.append(clf.score(X_test, y_test))\n",
    "    \n",
    "plt.title(\"Optimal n-estimators\", loc = 'left')\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "line = plt.plot(n_estimators, a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384c79e",
   "metadata": {},
   "source": [
    "From the the above graph we can observer optimal n_estimator is 65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimator = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10036167",
   "metadata": {},
   "source": [
    "2. Find max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6866f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [1, 2, 4, 8, 16, 32, 64, 100]\n",
    "y_transformed = lab.fit_transform(y.values.flatten())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.25)\n",
    "\n",
    "a=[]\n",
    "for depth in max_depths:\n",
    "    clf=RandomForestClassifier(n_estimators=n_estimator,max_depth=depth,min_samples_split=2,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    a.append(clf.score(X_test, y_test))\n",
    "    \n",
    "plt.title(\"Optimal Max depth\", loc = 'left')\n",
    "plt.xlabel(\"max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "line = plt.plot(max_depths, a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053ab46",
   "metadata": {},
   "source": [
    "From the above graph we can conclude 35 is optimal max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab84d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100babf",
   "metadata": {},
   "source": [
    "3. Find min_samples_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e558f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [2, 4, 8, 16]\n",
    "y_transformed = lab.fit_transform(y.values.flatten())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.25)\n",
    "\n",
    "a=[]\n",
    "for split in splits:\n",
    "    clf=RandomForestClassifier(n_estimators=n_estimator,max_depth=depth,min_samples_split=split,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    a.append(clf.score(X_test, y_test))\n",
    "\n",
    "plt.title(\"Optimal Tree Node Split\", loc = 'left')\n",
    "plt.xlabel(\"Number of splits\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "line = plt.plot(splits, a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023bfe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8297f6",
   "metadata": {},
   "source": [
    "Using the optimal parameters, we can train the model and cross validate the predictions using stratified KFold splits to split the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb476428",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_transformed = lab.fit_transform(y.values.flatten())\n",
    "skf = StratifiedKFold(n_splits=6,random_state=None,shuffle=True)\n",
    "# skf = RepeatedKFold(n_splits=6, n_repeats=2, random_state=42)\n",
    "\n",
    "a = []\n",
    "for train_index,test_index in skf.split(X,y_transformed):\n",
    "    print(f\"Train:{train_index} Test:{test_index}\")\n",
    "    X_train,X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train,y_test = y_transformed[train_index], y_transformed[test_index]\n",
    "    clf=RandomForestClassifier(n_estimators=n_estimator,max_depth=depth,min_samples_split=split,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    a.append(clf.score(X_test, y_test))\n",
    "    \n",
    "sum=0\n",
    "for aa in a:\n",
    "    sum+=aa\n",
    "print(f\"mean score from K-fold cross validation: {sum/len(a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ef676",
   "metadata": {},
   "source": [
    "The results of the evaluations will be discussed in Evaluation section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef2ea2",
   "metadata": {},
   "source": [
    "Also used cross_val_score to find the accurace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf, X, y_transformed, scoring='accuracy', cv = 6)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a280a7f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4991302",
   "metadata": {},
   "source": [
    "We can see the results in confusion matrix of the results and classification reports. The accuracy score is 98% using random forest with cross validations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af2ebbd",
   "metadata": {},
   "source": [
    "Now we can visualise the feature importance of the random classifier. Also to note, Using the feauture importance on the previous runs i have removed the year column since it was having less importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3190bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_imp = pd.Series(clf.feature_importances_,\\\n",
    "                        index=X_train.columns).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c36b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629781f2",
   "metadata": {},
   "source": [
    "We can observe that speed is having high importance. On the contrary, vessel course is having less importance in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42e3f4",
   "metadata": {},
   "source": [
    "### 5.2 K-Nearest Neighbors <a class=\"anchor\" id=\"knn\"></a>\n",
    "\n",
    " To find the optimal n neighbors. I have used GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "y_transformed = lab.fit_transform(y.values.flatten())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.25)\n",
    "\n",
    "parameters = {\"n_neighbors\": range(1, 50,)}\n",
    "gridsearch = GridSearchCV(KNeighborsClassifier(),  parameters,cv=2,n_jobs=-1)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a123a8",
   "metadata": {},
   "source": [
    "The optimal neighbors is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccfaee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e2245",
   "metadata": {},
   "source": [
    "Now we using stratified KFold data splits we tarin and predict the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f718bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_transformed = lab.fit_transform(y.values.flatten())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.25,random_state=109)\n",
    "#skf = KFold(n_splits=5,random_state=None,shuffle=True)\n",
    "# skf = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "skf = StratifiedKFold(n_splits=6,random_state=None,shuffle=True)\n",
    "a = []\n",
    "for train_index,test_index in skf.split(X,y_transformed):\n",
    "    print(f\"Train:{train_index} Test:{test_index}\")\n",
    "    X_train,X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train,y_test = y_transformed[train_index], y_transformed[test_index]\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=n_neighbors,n_jobs=-1)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred=knn_model.predict(X_test)\n",
    "    a.append(knn_model.score(X_test, y_test))\n",
    "    print()\n",
    "sum=0\n",
    "for aa in a:\n",
    "    sum+=aa\n",
    "print(f\"mean score from K-fold cross validation: {sum/len(a)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5556968",
   "metadata": {},
   "source": [
    "The KNN model with 2 neighbors, accuracy score is 95%. !. However the mean of scores in cross validations is 60%. We will discuss this in detail in evaluation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c2cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(clf, X, y_transformed, scoring='accuracy', cv = 2)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd533a6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b37649",
   "metadata": {},
   "source": [
    "### 5.3 Gaussian Naive Bayes <a class=\"anchor\" id=\"gnb\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5cf06",
   "metadata": {},
   "source": [
    "Using Stratified KFold we split the data to train predict the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "y = fishing_vessels[target]\n",
    "y_transformed = lab.fit_transform(y.values.flatten())\n",
    "y[target].apply(lambda x: 1 if x.is_fishing>=0.5 else 0, axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.25,random_state=109)\n",
    "#skf = KFold(n_splits=5,random_state=None,shuffle=True)\n",
    "# skf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=42)\n",
    "skf = StratifiedKFold(n_splits=6,random_state=None,shuffle=True)\n",
    "a = []\n",
    "for train_index,test_index in skf.split(X,y_transformed):\n",
    "    print(f\"Train:{train_index} Test:{test_index}\")\n",
    "    X_train,X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train,y_test = y_transformed[train_index], y_transformed[test_index]\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train,y_train)\n",
    "    y_pred=gnb.predict(X_test)\n",
    "    a.append(gnb.score(X_test, y_test))\n",
    "    print()\n",
    "sum=0\n",
    "for aa in a:\n",
    "    sum+=aa\n",
    "print(f\"mean score from K-fold cross validation: {sum/len(a)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283cfddc",
   "metadata": {},
   "source": [
    "The mean score is 61%!. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97c20c",
   "metadata": {},
   "source": [
    "Also the accuracy score for 10 fold cross validations is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01519f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(gnb, X, y_transformed, scoring='accuracy', cv = 2)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123643f",
   "metadata": {},
   "source": [
    "Confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb90a467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c50379",
   "metadata": {},
   "source": [
    "### 5.4 Logistic Regression <a class=\"anchor\" id=\"lr\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d8c4af",
   "metadata": {},
   "source": [
    "Using Startified KFold data split to train and predict the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afefec70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X = fishing_vessels[features]\n",
    "y = fishing_vessels[target]\n",
    "# y[target].apply(lambda x: 1 if x.is_fishing>=0.5 else 0, axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values.flatten(), test_size=0.25,random_state=109)\n",
    "#skf = KFold(n_splits=5,random_state=None,shuffle=True)\n",
    "# skf = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)\n",
    "skf = StratifiedKFold(n_splits=6,random_state=None,shuffle=True)\n",
    "a = []\n",
    "for train_index,test_index in skf.split(X,y_transformed):\n",
    "    print(f\"Train:{train_index} Test:{test_index}\")\n",
    "    X_train,X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train,y_test = y_transformed[train_index], y_transformed[test_index]\n",
    "    lr = LogisticRegression(multi_class='multinomial',solver='lbfgs',max_iter=20000)\n",
    "    lr.fit(X_train,y_train)\n",
    "    y_pred=lr.predict(X_test)\n",
    "    a.append(lr.score(X_test, y_test))\n",
    "    print()\n",
    "sum=0\n",
    "for aa in a:\n",
    "    sum+=aa\n",
    "print(f\"mean score from K-fold cross validation: {sum/len(a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ee30e",
   "metadata": {},
   "source": [
    "The mean accuracy score is 64% !. However the convergence having a issue even if the solver is changed and max_iter is changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689958a",
   "metadata": {},
   "source": [
    "The accuracy scores of 10 fold cross validations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d45a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(lr, X, y_transformed, scoring='accuracy', cv = 6)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba30115",
   "metadata": {},
   "source": [
    "Confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781cc29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91b4b1",
   "metadata": {},
   "source": [
    "### 5.5 Neural Networks <a class=\"anchor\" id=\"nn\"></a>\n",
    "\n",
    "I have used sequential neural network model to do regression on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9a9e0",
   "metadata": {},
   "source": [
    "Importing tesorflow-gpu libraries for trainig neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c56f4",
   "metadata": {},
   "source": [
    "Set the tesorflow session and enable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35192331",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a81829",
   "metadata": {},
   "source": [
    "List all CPUs and GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a3f73",
   "metadata": {},
   "source": [
    "Transform features data into standard Scalar to help fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fishing_vessels[features]\n",
    "y = fishing_vessels[target]\n",
    "# y[target].apply(lambda x: 1 if x.is_fishing>0 else 0, axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values.flatten(), test_size=0.25,random_state=109)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=109)\n",
    "def scale_datasets(x_train, x_test):\n",
    "    standard_scaler = StandardScaler()\n",
    "    x_train_scaled = pd.DataFrame(standard_scaler.fit_transform(X_train),columns=x_train.columns)\n",
    "    x_test_scaled = pd.DataFrame(standard_scaler.transform(X_test),columns = x_test.columns)\n",
    "    return x_train_scaled, x_test_scaled\n",
    "x_train_scaled, x_test_scaled = scale_datasets(X_train, X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb841dd4",
   "metadata": {},
   "source": [
    "msle = MeanSquaredLogarithmicError()\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(160,  kernel_initializer='normal', activation='sigmoid'))\n",
    "# model.Dropout(0.2)\n",
    "model.add(Dense(480, kernel_initializer='normal', activation='sigmoid'))\n",
    "# model.Dropout(0.2)\n",
    "model.add(Dense(256,  kernel_initializer='normal',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(x_train_scaled, y_train, epochs=10, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(x_test_scaled, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431f5e7",
   "metadata": {},
   "source": [
    "3 dense layes are used in the neural network and model is built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34321ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units1 = 160\n",
    "hidden_units2 = 480\n",
    "hidden_units3 = 256\n",
    "learning_rate = 0.01\n",
    "# Creating model using the Sequential in tensorflow\n",
    "def build_model_using_sequential():\n",
    "  model = Sequential([\\\n",
    "    Dense(hidden_units1, kernel_initializer='normal', activation='sigmoid'),\\\n",
    "    Dropout(0.2),\\\n",
    "    Dense(hidden_units2, kernel_initializer='normal', activation='sigmoid'),\\\n",
    "    Dropout(0.2),\\\n",
    "    Dense(hidden_units3, kernel_initializer='normal', activation='sigmoid'),\\\n",
    "    Dense(1, kernel_initializer='normal', activation='linear')\\\n",
    "  ])\n",
    "  return model\n",
    "# build the model\n",
    "model = build_model_using_sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b188da",
   "metadata": {},
   "source": [
    "Loss function is set and model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e28916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "msle = MeanSquaredLogarithmicError()\n",
    "model.compile(\n",
    "    loss=msle, \n",
    "    optimizer=Adam(learning_rate=learning_rate), \n",
    "    metrics=['accuracy',msle]\n",
    ")\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    x_train_scaled.values, \n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "accuracy = model.evaluate(x_test_scaled, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, key):\n",
    "  plt.plot(history.history[key])\n",
    "  plt.plot(history.history['val_'+key])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(key)\n",
    "  plt.legend([key, 'val_'+key])\n",
    "  plt.show()\n",
    "# Plot the history\n",
    "# binary_crossentropy\n",
    "# mean_squared_logarithmic_error\n",
    "plot_history(history, 'mean_squared_logarithmic_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd5d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test_scaled, y_test, verbose=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1039012",
   "metadata": {},
   "source": [
    "The error is low when for 4 epocs. However, its not a steady graph and its not good as other ML model used above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f2fcd",
   "metadata": {},
   "source": [
    "## 6. Evaluation <a class=\"anchor\" id=\"sixth-bullet\"></a>\n",
    "\n",
    "1. Random Forest<br>\n",
    "    The evaluation is done by finding optimal parameters one by one and using stratified KFold data split resulting in mean accuracy of 98%. However, when shuffle is set to false the model perform very badly by giving 58% score. This shows the that the data is balanced for the model and can produce better predictions.\n",
    "    \n",
    "2. K-Nearest Neighbours<br>\n",
    "    The optimal neighbor is genreated by GridSearch and using Startified KFold data splits the model is able to give the mean accuracy score of 60%. However, the acuracy score when test with all test data gives 95%. Also like other models when shuffle is set to false the model performs poorly. \n",
    "    \n",
    "3. Naive Bayes<br>\n",
    "    The model is evaluated by using Stratified KFold data split and the mean accuracy scores is 61%.\n",
    "4. Logistic Regression<br>\n",
    "    The model generates the mean accuracy score of 64% using startified Kfold data split technique. However, the model did not converge properly as expected. I have tried to increase the max_depth and solver function to 'saga' but the model us still not converging to the gradient descent. So this model may not be accurate.\n",
    "5. Neural Networks\n",
    "    The model is generated with 3 dense layer in the network but the prediction and the mean erros are not consistent while running it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f2416",
   "metadata": {},
   "source": [
    "## 7. Deployments <a class=\"anchor\" id=\"seventh-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1acdc5a",
   "metadata": {},
   "source": [
    "All models are deployed in my machine with use of GPU (neural netowrk - tensor flow) and parallel processing using n_jobs=-1 in models from scikit learn. Only, neural netowrk workload was high due to the dense layes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d477a6d",
   "metadata": {},
   "source": [
    "## 8. Discussion <a class=\"anchor\" id=\"eighth-bullet\"></a>\n",
    "\n",
    "* Random forest model produces confident predictions since the data imbalance was kept to lower by tweaking the algorithm. On the other hand, when the model trained with default parameter gives low accuracy scores of 94%.\n",
    "* KNN algorithm performed quickest of all algorithms using n_jobs=-1 for parallel threads. The accuracy scores is the second best of all 5 models. However while cross validation the accuracy score drops to 60%. So, This may be caused by the model to overfit with all the test data and this model cant be used in real time.\n",
    "* Naive Bayes produced 61% accuracy score and is the fourth best of five models. Naive Bayes was also quick . However, iterating with KFold technique increased the overall time.\n",
    "* Logistic Regression model produced 64% accuracy score the third best of five models. However the gradient descend was not working as expected. Even after tweaking the parameter, i could still not able to fix the model. Making the max_iter to 4000 might work but the process kept running. So this model is not a trustable model for prediction of the fishing activities data\n",
    "* Neural networks produced very extreme results while running it different times. Also, the network is set to be big for regression and leverages the performance from GPUs. However, the process waits and halts making it more time cinsuming to train and tweak. In future moving the workload to cloud machines may help tweak the model with consistent accuracy results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8908dbc",
   "metadata": {},
   "source": [
    "## 9. Result <a class=\"anchor\" id=\"nineth-bullet\"></a>\n",
    "\n",
    "After, experimenting with five machine learning algorithms, clearly Random Forest model performs better with 98% accuracy with cross validations. Future scope of this project is to classify the vessel type from the data and find possible illegal fishing activities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84059322",
   "metadata": {},
   "source": [
    "## 9. References <a class=\"anchor\" id=\"tenth-bullet\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186cac9a",
   "metadata": {},
   "source": [
    "1. Sans, P. and Combris, P., 2015. World meat consumption patterns: An overview of the last fifty years (1961–2011). Meat Science, 109, pp.106-111.\n",
    "2. Globalfishingwatch.org. 2022. GFW | Map. [online] Available at: <https://globalfishingwatch.org/map/> [Accessed 29 April 2022].\n",
    "3. Global Fishing Watch | Data download portal. 2022. Global Fishing Watch | Data download portal. [online] Available at: <https://globalfishingwatch.org/data-download/> [Accessed 29 April 2022].\n",
    "4. Sustainable Fisheries UW. 2022. What does the world eat? - Sustainable Fisheries UW. [online] Available at: <https://sustainablefisheries-uw.org/seafood-101/what-does-the-world-eat/> [Accessed 29 April 2022].\n",
    "5. Kroodsma, D.A., Mayorga, J., Hochberg, T., Miller, N.A., Boerder, K., Ferretti, F., Wilson, A., Bergman, B., White, T.D., Block, B.A. and Woods, P., 2018.\n",
    "6. Improving fishing pattern detection from satellite AIS using data mining and machine learning. de Souza, E.N., Boerder, K., Matwin, S. and Worm, B., 2016.\n",
    "\n",
    "Tutorial Websites:\n",
    "\n",
    "https://realpython.com/knn-python/\n",
    "https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
    "https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d\n",
    "https://www.analyticsvidhya.com/blog/2021/05/4-ways-to-evaluate-your-machine-learning-model-cross-validation-techniques-with-python-code/\n",
    "https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right\n",
    "https://neptune.ai/blog/how-to-deal-with-imbalanced-classification-and-regression-data\n",
    "https://machinelearningmastery.com/repeated-k-fold-cross-validation-with-python/\n",
    "https://www.analyticsvidhya.com/blog/2021/08/a-walk-through-of-regression-analysis-using-artificial-neural-networks-in-tensorflow/\n",
    "https://towardsdatascience.com/is-a-trawler-fishing-modelling-the-global-fishing-watch-dataset-d1ffb3e7624a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834bd5f9",
   "metadata": {},
   "source": [
    "################################################# END of FILE ###########################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
